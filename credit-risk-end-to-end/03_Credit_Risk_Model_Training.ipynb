{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f2ab19-68e1-4725-b6e7-efd8eedebe1a",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/feast-dev/feast/master/docs/assets/feast_logo.png width=400/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a40de4-65cf-4b45-b321-2b7ce571f8cb",
   "metadata": {},
   "source": [
    "# Credit Risk Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe641d83-1e28-4f7f-895c-8ca038f6cc53",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04f635-401b-47b6-b807-df61d42ec752",
   "metadata": {},
   "source": [
    "AI models have played a central role in modern credit risk assessment systems. In this example, we develop a credit risk model to predict whether a future loan will be good or bad, given some context data (presumably supplied from the loan application process). We use the modeling process to demonstrate how Feast can be used to facilitate the serving of data for training and inference use-cases.\n",
    "\n",
    "In this notebook, we train our AI model. We will use the popular scikit-learn library (sklearn) to train a RandomForestClassifier, as this is a relatively easy choice for a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bf1aa-c450-4201-83a4-e25b08bdd12d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b33bc-bc06-4de0-8f3a-beea8179035c",
   "metadata": {},
   "source": [
    "*The following code assumes that you have read the example README.md file, and that you have setup an environment where the code can be run. Please make sure you have addressed the prerequisite needs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a3dab-fdbf-40be-8227-6180dc314a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "import datetime\n",
    "import feast\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from feast import FeatureStore, RepoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a841445-fa47-4826-a874-28ac0e4ea57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23579727-7797-4101-a70d-b0d4c24b0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "SEED = 142"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5be519-7733-449b-8dc3-411e86371315",
   "metadata": {},
   "source": [
    "This notebook assumes that you have already done the following:\n",
    "\n",
    "1. Run the [01_Credit_Risk_Data_Prep.ipynb](01_Credit_Risk_Data_Prep.ipynb) notebook to prepare the data.\n",
    "2. Run the [02_Deploying_the_Feature_Store.ipynb](02_Deploying_the_Feature_Store.ipynb) notebook to configure the feature stores and launch the feature store servers.\n",
    "\n",
    "If you have not completed the above steps, please go back and do so before continuing. This notebook relies on the data prepared by 1, and it uses the Feast offline server stood up by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca99047-e508-4b1f-9f4c-f11e38587d70",
   "metadata": {},
   "source": [
    "### Load Label (Outcome) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b49268-b7a5-4abc-8d82-1cdbf9bb4473",
   "metadata": {},
   "source": [
    "From our previous data exploration, remember that the label data represents whether the loan was classed as \"good\" (1) or \"bad\" (0). Let's pull the labels for training, as we will use them as our \"entity dataframe\" when pulling features.\n",
    "\n",
    "This is also a good time to remember that the label timestamps are lagged by 30-90 days from the context data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a227a12-7b3e-462a-8f6e-38a7690df1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_parquet(\"Feature_Store/data/labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a39cad-0a85-4d98-ad95-008c81bb6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f29fd-46d3-444b-b24f-eaccd82ab7d3",
   "metadata": {},
   "source": [
    "### Pull Feature Data from Feast Offline Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c13b69-3d26-484c-97cd-97734cc812bd",
   "metadata": {},
   "source": [
    "In order to pull feature data from the offline store, we create a FeatureStore object that connects to the offline server (continuously running in the previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9828f8-f210-4586-ac36-3f7e17f4f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FeatureStore object\n",
    "# (connects to the offline server deployed in 02_Deploying_the_Feature_Store.ipynb) \n",
    "store = FeatureStore(config=RepoConfig(\n",
    "    project=\"loan_applications\",\n",
    "    provider=\"local\",\n",
    "    registry=\"Feature_Store/data/registry.db\",\n",
    "    offline_store={\n",
    "        \"type\": \"remote\",\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 8815\n",
    "    },\n",
    "    entity_key_serialization_version=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007e7ca-40c1-4850-abed-73b6171ad08d",
   "metadata": {},
   "source": [
    "Now, we can retrieve feature data by supplying our entity dataframe and feature specifications to the `get_historical_features` function. Note that this function performs a fuzzy lookback (\"point-in-time\") join, matching the lagged outcome timestamp to the closest application timestamp (per ID) in the context data; it also joins the \"a\" and \"b\" features that we had previously split into two tables.\n",
    "\n",
    "To keep this example simple, we will limit our feature set to the numerical features plus two categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e3cb5-c865-48f4-80b6-8a14a1ff09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature data\n",
    "# (Joins a and b data, and selects records with the right timestamps)\n",
    "df = store.get_historical_features(\n",
    "    entity_df=labels,\n",
    "    features=[\n",
    "        \"data_a:duration\",\n",
    "        \"data_a:credit_amount\",\n",
    "        \"data_a:installment_commitment\",\n",
    "        \"data_a:checking_status\",\n",
    "        \"data_b:residence_since\",\n",
    "        \"data_b:age\",\n",
    "        \"data_b:existing_credits\",\n",
    "        \"data_b:num_dependents\",\n",
    "        \"data_b:housing\"\n",
    "    ]\n",
    ").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f6cb1-bbbf-4512-98cd-0abe5ff0c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ea48c-0a5a-4642-aaba-a9eeb4a7da48",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6669dce-a8b0-4d80-9a15-70b7dfd2d718",
   "metadata": {},
   "source": [
    "Next, we split the data into a `train` and `validate` set, which we will use to train and then validate a model. The validation set will allow us to more accurately assess the model's performance on data that it has not seen during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b0a54-48e4-4414-bb8c-0c30b6ab7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validate datasets\n",
    "train, validate = train_test_split(df, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65cbf7-5981-4f51-97aa-a3ff7027f2f3",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516ded8-10ad-4274-a736-f288290b5883",
   "metadata": {},
   "source": [
    "Before building a model, a data scientist needs to gain understanding of the data to make sure it meets important statistical assumptions, and to identify potential opportunities and issues. As the purpose of this particular example is to show working with Feast, we will take the view of a data scientist looking to build a quick baseline model to establish some low-end metrics.\n",
    "\n",
    "Note that this data set is very \"clean\", as it has already been prepared. In real-life, production credit risk data can be much more complex, and have many issues that need to be understood and addressed before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553986a0-c804-4ab4-a4b9-48b16c72fd4f",
   "metadata": {},
   "source": [
    "Let's look at counts for the target variable `class`, which tells us whether a (historical) loan was good or bad. We can see that there were many more good loans than bad, making the dataset imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bd29b-eaf4-41a6-aaca-a8eaaf37e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the target variable \"class\"\n",
    "p = sns.histplot(train[\"class\"], ec=\"w\", lw=4)\n",
    "_ = p.set_title(\"Bad vs. Good Loan Count\")\n",
    "_ = p.spines[\"top\"].set_visible(False)\n",
    "_ = p.spines[\"right\"].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a697a5-5709-4a69-b644-62779b4f8bc5",
   "metadata": {},
   "source": [
    "Now, view the first few records of the context data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79424785-129d-4007-84a5-041b6d38457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first records in training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52f5bc-aa0f-48db-b356-c52aa7ce3724",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b5c02-ad4d-400e-bdac-bfdf2799f575",
   "metadata": {},
   "source": [
    "Once data columns have been prepared so that they can be used to train an AI model, it is common to refer to them as \"features\". The process of preparing features is referred to as \"feature engineering\". \n",
    "\n",
    "Below, we will train a random forest model. Random forests are relatively robust to non-standardized, non-normalized data, making it easier for us to getting started. As such, the numerical columns are ready for a simple baseline training. \n",
    "\n",
    "We have pulled two categorical columns, wich we will need to engineer into numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6fb27-140c-4f5a-b464-1f5e5d81d086",
   "metadata": {},
   "source": [
    "The `checking_status` column tells us roughly how much money the applicant has in their checking account, while the `housing` column shows the applicant's housing status. We presume that more money in checking correlates inversely with credit risk, while owing vs. renting, vs. living for free correlates directly with credit risk. Hence, converting these to ordinal features makes sense. Of course, in a real study we would want to quantitatively verify these presumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e374096-b02d-4cbb-8fca-dcc451c90c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the `checking_status` column distibution\n",
    "train.checking_status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144b525-244b-4526-8e4b-d393cb174d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the `housing` column distribution\n",
    "train.housing.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb340b4-7d21-4810-8be2-1633da2e4396",
   "metadata": {},
   "source": [
    "We define a tranformer that can be used to convert `checking_status` and `housing` to ordinal variables. The transformer will also drop the non-feature columns (`class`, `ID`, and `application_timestamp`) from the feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27796e23-c12e-4e51-8fb4-090b26aff2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature lists\n",
    "cat_features = [\"checking_status\", \"housing\"]\n",
    "num_features = [\n",
    "    \"duration\", \"credit_amount\", \"installment_commitment\",\n",
    "    \"residence_since\", \"age\", \"existing_credits\", \"num_dependents\"\n",
    "]\n",
    "\n",
    "# Ordinal encoder for cat_features\n",
    "# (We use a ColumnTransformer to passthrough numerical feature columns)\n",
    "col_transform = ColumnTransformer([\n",
    "        (\"cat_features\", OrdinalEncoder(), cat_features),\n",
    "        (\"num_features\", \"passthrough\", num_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318429b9-e008-4cc7-8108-779934f9ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tranform outputs features as expected\n",
    "# (Note: transform output is an array, so we convert it\n",
    "# back to dataframe for inspection)\n",
    "pd.DataFrame(\n",
    "    index=train.index,\n",
    "    columns=cat_features + num_features,\n",
    "    data= col_transform.fit_transform(train)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3785c93-8830-4fa2-bb9d-31b6e8fecb01",
   "metadata": {},
   "source": [
    "Finally, let's separate out the labels, and engineer them from categorical (\"good\" | \"bad\") to float (1.0 | 0.0). We do this for both the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebff90-a193-43a2-86fb-cf09e7d03777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make \"class\" target variable numeric\n",
    "train_y = (train[\"class\"] == \"good\").astype(float)\n",
    "validate_y = (validate[\"class\"] == \"good\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052f6b2-2a34-441d-8a5f-2aad4e4db022",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f14590-31f4-4680-b1a1-75755a78513e",
   "metadata": {},
   "source": [
    "Now that the features are prepared, we can train (fit) our baseline model on the feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff48f34-dbb6-4221-aefc-3c9b3f9da3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    criterion=\"entropy\",\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight={0:5, 1:1},\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Package transform and model in pipeline\n",
    "model = Pipeline([(\"transform\", col_transform), (\"rf_model\", rf_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ef38a-23b0-4056-a108-960495521164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c45c39-9d8e-4f76-aca5-9f0c1568d263",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58d432-80ba-428f-b59f-621a9e53b331",
   "metadata": {},
   "source": [
    "Let's evaluate our baseline model performance. With credit risk, recall is going to be an important measure to look at. We compare the performance on the training data, with the performance on the validation data through a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5472f6-2ddc-437d-8102-4d5bd2c9f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training set performance\n",
    "train_preds = model.predict(train)\n",
    "print(classification_report(train_y, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296bbd3-603e-4615-abbe-2689ebcf5d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate validation data performance\n",
    "print(classification_report(validate_y, model.predict(validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ffbdc-f0b3-4fb6-9575-5acd983082cf",
   "metadata": {},
   "source": [
    "The recall on the validation set for bad loans (0 class) is 0.87, meaning that the model correctly identified close to 90% of the bad loans. However, the precision of 0.46 tells us that the model is also classifying many loans that were actually good as bad. Precision and recall are technical metrics. In order to truly assess the models value, we would need feedback from the business side on the impact of misclassifications (for both good and bad loans).\n",
    "\n",
    "The difference in performance on the training vs. validation data, tells us that the model is slightly overfitting the data. Remember that this is just a quick baseline model. To improve further, we could do things like:\n",
    "- gather more data\n",
    "- engineer features\n",
    "- experiment with hyperparameter settings\n",
    "- experiment with other model types\n",
    "\n",
    "In fact, this is just a start. Creating AI models that meet business needs often requires a lot of guided experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378d21a-d6db-42f9-851a-ce71f68c6802",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450a328-f00c-4579-8e08-b2ebe5046961",
   "metadata": {},
   "source": [
    "The last thing we do is save our trained model, so that we can pick it up later in the serving environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a7906-d54f-4f2d-9803-6c82c86b28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a pickle file\n",
    "joblib.dump(model, \"rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299588b8-ab67-4155-97a9-770e8e4a7476",
   "metadata": {},
   "source": [
    "In the next notebook, [04_Credit_Risk_Model_Serving.ipynb](04_Credit_Risk_Model_Serving.ipynb), we will load the trained model and request predictions, with input features provided by the Feast online feature server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
